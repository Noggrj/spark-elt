# ğŸš€ Projeto ELT com PySpark, Kafka e Streaming

## ğŸ“‹ Sobre o Projeto
Este projeto implementa uma pipeline de dados em tempo real utilizando tecnologias modernas de Big Data. A arquitetura Ã© baseada em um fluxo ELT (ExtraÃ§Ã£o, Carregamento e TransformaÃ§Ã£o) que processa dados de clientes e transaÃ§Ãµes atravÃ©s de um sistema distribuÃ­do.

O pipeline Ã© composto por:

- ExtraÃ§Ã£o e validaÃ§Ã£o de dados com Pandas
- GeraÃ§Ã£o de dados sintÃ©ticos com Faker para testes e desenvolvimento
- Streaming em tempo real com Apache Kafka
- Processamento distribuÃ­do com Apache Spark
- VisualizaÃ§Ã£o atravÃ©s de interface grÃ¡fica para Kafka
Tudo isso Ã© orquestrado em contÃªineres Docker, garantindo portabilidade e escalabilidade.

---

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ start_spark.py               # Ponto de entrada para execuÃ§Ã£o via Docker
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                     # ContÃ©m os CSVs de entrada (clientes e transaÃ§Ãµes)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ extract_clientes.py
â”‚   â”‚   â””â”€â”€ extract_transacoes.py
â”‚   â””â”€â”€ transform/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ spark_processing.py
```

---

## ğŸ§± Tecnologias Utilizadas

- [Python 3.11+](https://www.python.org/)
- [Apache Spark 3+ (via PySpark)](https://spark.apache.org/)
- [Apache Kafka](https://kafka.apache.org/) - Plataforma de streaming distribuÃ­do
- [Zookeeper](https://zookeeper.apache.org/) - CoordenaÃ§Ã£o de serviÃ§os distribuÃ­dos
- [Flask](https://flask.palletsprojects.com/) - Framework web para API REST
- [Pandas](https://pandas.pydata.org/)
- [Faker](https://faker.readthedocs.io/)
- [Docker](https://www.docker.com/) e [Docker Compose](https://docs.docker.com/compose/)
- [Jupyter Base Notebook (imagem Docker)](https://hub.docker.com/r/jupyter/pyspark-notebook)


---

## ğŸ“¦ InstalaÃ§Ã£o e ExecuÃ§Ã£o (via Docker)

### 1. Build da imagem

```bash
make build
```

### 2. Inicie os serviÃ§os e Execute o Pipeline

```bash
make start
```
### 3. Verifique os logs

## ğŸ“‚ Fontes de Dados

Os dados de entrada estÃ£o localizados em `data/raw/`:

- `clientes.csv`
- `transacoes.csv`

Esses arquivos sÃ£o validados antes da transformaÃ§Ã£o com Spark:

- Checagem de colunas obrigatÃ³rias  
- Unicidade de chaves  
- ValidaÃ§Ã£o de formato de datas  

---

## ğŸ”„ Pipeline

### ğŸ”¹ Etapa Extract (pandas)
- Valida dados brutos com `pandas`
- Verifica integridade e estrutura dos arquivos

### ğŸ”¸ Etapa Transform (PySpark)
- Cria colunas derivadas (`ano`, `mÃªs`, `faixa_etaria`)
- Agrega dados por categoria, cidade e cliente
- Executa consultas SQL no Spark

---


## ğŸ§° Comandos Ãºteis (Makefile)

```bash
make build           # Build da imagem Docker
make start           # Executa o pipeline via Docker
make lint            # Verifica estilo de cÃ³digo com flake8, isort
make lint-fix        # Aplica formataÃ§Ã£o com black, isort
make check-init      # Verifica arquivos __init__.py nas pastas
```

## ğŸŒ Webservice Kafka

O projeto agora inclui um webservice para interagir com o Kafka atravÃ©s de uma API REST.


## ğŸ§‘â€ğŸ’» Autor
Desenvolvido por Matheus Nogueira

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Consulte o arquivo `LICENSE` para mais informaÃ§Ãµes.
